# -*- coding: utf-8 -*-
"""timePytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jmHBoi_qNsqIp04ztlABIcrfmLnjwV4t
"""

import numpy as np
import pandas as pd

# ØªØ¹Ø±ÛŒÙ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§
num_samples = 1000

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø«Ø§Ø¨Øª
np.random.seed(42)  # Ø«Ø§Ø¨Øª Ú©Ø±Ø¯Ù† Seed Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù‚Ø§Ø¨Ù„ ØªÚ©Ø±Ø§Ø±
MW = 16.04  # ÙˆØ²Ù† Ù…ÙˆÙ„Ú©ÙˆÙ„ÛŒ Ú¯Ø§Ø² (kg/kmol)
R = 8.314 / MW  # Ø«Ø§Ø¨Øª Ú¯Ø§Ø² Ø¬Ù‡Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø§Ø² Ù…ØªØ§Ù† (kJ/kgÂ·K)
gamma = 1.31  # Ù†Ø³Ø¨Øª Ú¯Ø±Ù…Ø§ÛŒ Ù…Ø®ØµÙˆØµ
Cp = gamma * R / (gamma - 1)  # Ú¯Ø±Ù…Ø§ÛŒ Ù…Ø®ØµÙˆØµ Ø«Ø§Ø¨Øª ÙØ´Ø§Ø± (kJ/kgÂ·K)

# ØªØ¹Ø±ÛŒÙ Ø¨Ø±Ø¯Ø§Ø± Ø²Ù…Ø§Ù†ÛŒ
time_interval = 1  # ÙØ§ØµÙ„Ù‡ Ø²Ù…Ø§Ù†ÛŒ (Ø«Ø§Ù†ÛŒÙ‡)
time = np.arange(0, num_samples * time_interval, time_interval)  # Ø¨Ø±Ø¯Ø§Ø± Ø²Ù…Ø§Ù†ÛŒ

# ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ
pressure_in = 3.5 + 0.2 * np.sin(0.05 * time) + np.random.normal(0, 0.05, num_samples)  # ÙØ´Ø§Ø± ÙˆØ±ÙˆØ¯ÛŒ (bara)
temperature_in = 293 + 5 * np.cos(0.03 * time) + np.random.normal(0, 1, num_samples)  # Ø¯Ù…Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ (K)
flow_rate = 12 + 0.5 * np.sin(0.04 * time) + np.random.normal(0, 0.1, num_samples)  # Ø¬Ø±ÛŒØ§Ù† Ø¬Ø±Ù…ÛŒ (kg/s)
efficiency = 0.82 + 0.02 * np.sin(0.02 * time) + np.random.normal(0, 0.005, num_samples)  # Ú©Ø§Ø±Ø§Ù“ÛŒÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡
vibration = 1.0 + 0.3 * np.sin(0.06 * time) + np.random.normal(0, 0.05, num_samples)  # Ø§Ø±ØªØ¹Ø§Ø´Ø§Øª (mm/s)

# Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø±ÙˆØ§Ø¨Ø· ÙÛŒØ²ÛŒÚ©ÛŒ
pressure_ratio = 5.0 + 0.2 * np.sin(0.05 * time) + np.random.normal(0, 0.05, num_samples)  # Ù†Ø³Ø¨Øª ÙØ´Ø§Ø±
pressure_out = pressure_in * pressure_ratio  # ÙØ´Ø§Ø± Ø®Ø±ÙˆØ¬ÛŒ (bara)

# Ø¯Ù…Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
temperature_out = temperature_in * (pressure_ratio ** ((gamma - 1) / (gamma * efficiency)))

# ØªÙˆØ§Ù† Ù…ØµØ±ÙÛŒ
power_consumption = flow_rate * Cp * (temperature_out - temperature_in) / efficiency  # kW

# Ø§ÛŒØ¬Ø§Ø¯ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ùˆ Status Ø¨Ø§ ØªÙˆØ§Ø²Ù† Ù…Ù†Ø§Ø³Ø¨
# ØªÙ†Ø¸ÛŒÙ… Ø§Ù“Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ Ø¨Ù‡ Û¶ÛµÙª NormalØŒ Û²Û°Ùª ImbalanceØŒ Û±ÛµÙª Fault
normal_threshold = np.percentile(vibration, 65)
imbalance_threshold = np.percentile(vibration, 85)
power_threshold = np.percentile(power_consumption, 65)
fault_power_threshold = np.percentile(power_consumption, 85)

status = np.where(
    (vibration < normal_threshold) & (power_consumption < power_threshold) & (efficiency > 0.80),
    "Normal",
    np.where(
        ((vibration >= normal_threshold) & (vibration < imbalance_threshold)) |
        ((power_consumption >= power_threshold) & (power_consumption < fault_power_threshold)),
        "Imbalance",
        "Fault"
    )
)

# Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø§Ø±ØªØ¹Ø§Ø´Ø§Øª
frequency = 50 + 10 * np.sin(0.03 * time) + np.random.normal(0, 2, num_samples)  # ÙØ±Ú©Ø§Ù†Ø³ Ø§Ø±ØªØ¹Ø§Ø´Ø§Øª (Hz)
amplitude = 0.5 + 0.2 * np.sin(0.04 * time) + np.random.normal(0, 0.05, num_samples)  # Amplitude Ø§Ø±ØªØ¹Ø§Ø´Ø§Øª (mm)
phase_angle = 180 + 30 * np.sin(0.02 * time) + np.random.normal(0, 5, num_samples)  # Ø²Ø§ÙˆÛŒÙ‡ ÙØ§Ø² (Degrees)

# Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…Ø¹Ø§Ø¯Ù„Ù‡ Ø¬Ø±Ù…-ÙÙ†Ø±
mass = 75 + 5 * np.sin(0.01 * time) + np.random.normal(0, 1, num_samples)  # Ø¬Ø±Ù… Ù…Ø­ÙˆØ± (kg)
stiffness = 5e5 + 1e4 * np.sin(0.01 * time) + np.random.normal(0, 5e3, num_samples)  # Ø³Ø®ØªÛŒ ÙÙ†Ø± (N/m)
damping = 500 + 50 * np.sin(0.01 * time) + np.random.normal(0, 10, num_samples)  # Ø¶Ø±ÛŒØ¨ Ù…ÛŒØ±Ø§ÛŒÛŒ (Ns/m)

# Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…Ø¹Ø§Ø¯Ù„Ù‡ Ù†Ø§ÙˆÛŒØ±-Ø§Ø³ØªÙˆÚ©Ø³
density = 0.7 + 0.05 * np.sin(0.02 * time) + np.random.normal(0, 0.01, num_samples)  # Ú†Ú¯Ø§Ù„ÛŒ Ú¯Ø§Ø² (kg/mÂ³)
velocity = 30 + 5 * np.sin(0.03 * time) + np.random.normal(0, 1, num_samples)  # Ø³Ø±Ø¹Øª Ø¬Ø±ÛŒØ§Ù† (m/s)
viscosity = 1e-5 + 1e-6 * np.sin(0.02 * time) + np.random.normal(0, 1e-8, num_samples)  # Ø¶Ø±ÛŒØ¨-viscosity (PaÂ·s)

# Ø§ÛŒØ¬Ø§Ø¯ DataFrame
data = {
    "Time": time,  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ø±Ø¯Ø§Ø± Ø²Ù…Ø§Ù†ÛŒ
    "Pressure_In": pressure_in,
    "Temperature_In": temperature_in - 273.15,  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Â°C
    "Flow_Rate": flow_rate,
    "Pressure_Out": pressure_out,
    "Temperature_Out": temperature_out - 273.15,  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Â°C
    "Efficiency": efficiency,
    "Power_Consumption": power_consumption,
    "Vibration": vibration,
    "Status": status,
    "Frequency": frequency,
    "Amplitude": amplitude,
    "Phase_Angle": phase_angle,
    "Mass": mass,
    "Stiffness": stiffness,
    "Damping": damping,
    "Density": density,
    "Velocity": velocity,
    "Viscosity": viscosity
}

df = pd.DataFrame(data)

# Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Status
status_counts = df["Status"].value_counts(normalize=True) * 100
print("ØªÙˆØ²ÛŒØ¹ ÙˆØ¶Ø¹ÛŒØªâ€ŒÙ‡Ø§ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:")
print(status_counts)

# Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ CSV
df.to_csv("balanced_compressor_time_series_data.csv", index=False)

print("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø§Ø¯Ù„ Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù‚ÙˆÛŒ Ùˆ ØªÙˆØ²ÛŒØ¹ Ù…Ø·Ù„ÙˆØ¨ Status Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

data = pd.read_csv("/content/balanced_compressor_time_series_data.csv")
data.head()

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

# data.drop("Time", inplace=True,axis=1)
target = data['Vibration']
features = data[['Pressure_In',	'Temperature_In',	'Flow_Rate',	'Pressure_Out',	'Temperature_Out',	'Efficiency']]

target

features

scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)
scaled_target = scaler.fit_transform(target.values.reshape(-1, 1))

WINDOW_SIZE = 60  # ÛŒÚ© Ø³Ø§Ø¹Øª Ú¯Ø°Ø´ØªÙ‡ (60 Ø¯Ù‚ÛŒÙ‚Ù‡)
HORIZON = 1       # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÛŒÚ© Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡

# Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ù†Ø¬Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
def create_sequences(data, targets, window_size, horizon):
    X, y = [], []
    for i in range(len(data) - window_size - horizon + 1):
        X.append(data[i:i+window_size])
        y.append(targets[i+window_size+horizon-1])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_features, scaled_target, WINDOW_SIZE, HORIZON)

X

# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªÙ†Ø³ÙˆØ±Ù‡Ø§ÛŒ PyTorch
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ³Øª
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Ø§ÛŒØ¬Ø§Ø¯ DataLoader
batch_size = 64
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = TensorDataset(X_test, y_test)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

class TransformerModel(nn.Module):
    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers, dropout):
        super(TransformerModel, self).__init__()
        self.embed_dim = embed_dim
        self.embedding = nn.Linear(input_dim, embed_dim)

        # Ø§ÛŒØ¬Ø§Ø¯ Positional Encoding Ø³ÛŒÙ†ÙˆØ³ÛŒ
        position = torch.arange(WINDOW_SIZE).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-np.log(10000.0) / embed_dim))
        pe = torch.zeros(WINDOW_SIZE, embed_dim)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout
        )
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)
        self.fc_out = nn.Linear(embed_dim, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src):
        src = self.embedding(src) * np.sqrt(self.embed_dim)
        src = src + self.pe.unsqueeze(0)
        src = self.dropout(src)
        output = self.transformer_encoder(src)
        output = output.mean(dim=1)
        output = self.fc_out(output)
        return output

input_dim = X_train.shape[2]  # ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
embed_dim = 64
num_heads = 4
ff_dim = 128
num_layers = 2
dropout = 0.1

num_epochs = 20
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

from tqdm import tqdm
from sklearn.model_selection import ParameterGrid
import torch
import torch.nn as nn
import torch.optim as optim

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø«Ø§Ø¨Øª
input_dim = X_train.shape[2]  # ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Ø´Ø¨Ú©Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
param_grid = {
    'embed_dim': [64, 128],
    'num_heads': [4, 8],
    'ff_dim': [128, 256],
    'num_layers': [2, 3],
    'lr': [1e-4, 3e-4]
}

best_loss = float('inf')
best_params = None

# ØªØ¨Ø¯ÛŒÙ„ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ú¯Ø±ÛŒØ¯ Ø¨Ù‡ Ù„ÛŒØ³Øª
grid = list(ParameterGrid(param_grid))

# Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±
with tqdm(grid, desc="ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±", unit="Ù…Ø¯Ù„", colour='#00ff00') as pbar:
    for params in pbar:
        # Ù†Ù…Ø§ÛŒØ´ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÙØ¹Ù„ÛŒ
        pbar.set_postfix({
            'embed_dim': params['embed_dim'],
            'num_heads': params['num_heads'],
            'lr': params['lr']
        })

        # Ø³Ø§Ø®Øª Ù…Ø¯Ù„
        model = TransformerModel(
            input_dim=input_dim,
            embed_dim=params['embed_dim'],
            num_heads=params['num_heads'],
            ff_dim=params['ff_dim'],
            num_layers=params['num_layers'],
            dropout=0.1
        ).to(device)

        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´
        optimizer = optim.Adam(model.parameters(), lr=params['lr'])
        criterion = nn.MSELoss()

        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Early Stopping
        best_val_loss = float('inf')
        patience = 3
        trigger_times = 0

        # Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´
        with tqdm(range(20), desc="ğŸ¯ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„", leave=False, colour='#ffaa00') as epoch_pbar:
            for epoch in epoch_pbar:
                # Ø­Ø§Ù„Øª Ø¢Ù…ÙˆØ²Ø´
                model.train()
                train_loss = 0.0

                # Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ø¨Ú†â€ŒÙ‡Ø§
                for batch_X, batch_y in train_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)

                    optimizer.zero_grad()
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()

                    train_loss += loss.item()

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† loss Ø¢Ù…ÙˆØ²Ø´
                train_loss /= len(train_loader)

                # Ø­Ø§Ù„Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
                model.eval()
                val_loss = 0.0
                with torch.no_grad():
                    for batch_X, batch_y in test_loader:
                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                        outputs = model(batch_X)
                        val_loss += criterion(outputs, batch_y).item()

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† loss Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
                val_loss /= len(test_loader)

                # Ù†Ù…Ø§ÛŒØ´ Ù…Ù‚Ø§Ø¯ÛŒØ± loss
                epoch_pbar.set_postfix({
                    'train_loss': f"{train_loss:.4f}",
                    'val_loss': f"{val_loss:.4f}",
                    'patience': f"{trigger_times}/{patience}"
                })

                # Ø¨Ø±Ø±Ø³ÛŒ Early Stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    trigger_times = 0
                else:
                    trigger_times += 1
                    if trigger_times >= patience:
                        epoch_pbar.set_description("ğŸ›‘ ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…")
                        break

        # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        if best_val_loss < best_loss:
            best_loss = best_val_loss
            best_params = params
            pbar.set_postfix({
                'best_loss': f"{best_loss:.4f}",
                'best_embed_dim': best_params['embed_dim'],
                'best_lr': best_params['lr']
            })

# Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
print("\n" + "="*50)
print(f"ğŸ”¥ Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: \n{best_params}")
print(f"ğŸ“‰ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± Loss: {best_loss:.4f}")
print("="*50)

class EnhancedTransformer(nn.Module):
    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers, dropout):
        super(EnhancedTransformer, self).__init__()
        self.embed_dim = embed_dim
        self.embedding = nn.Linear(input_dim, embed_dim)

        # Positional Encoding Ù¾ÛŒØ´Ø±ÙØªÙ‡
        self.pos_encoder = PositionalEncoding(embed_dim, dropout)

        # Ø§ÙØ²ÙˆØ¯Ù† Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±
        encoder_layers = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)

        # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Fully Connected
        self.fc_layers = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, src):
        src = self.embedding(src) * np.sqrt(self.embed_dim)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src)
        output = output.mean(dim=1)
        output = self.fc_layers(output)
        return output

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ ÛŒØ§ÙØª Ø´Ø¯Ù‡ Ø§Ø² Grid Search
final_model = EnhancedTransformer(
    input_dim=input_dim,
    embed_dim=best_params['embed_dim'],
    num_heads=best_params['num_heads'],
    ff_dim=best_params['ff_dim'],
    num_layers=best_params['num_layers'],
    dropout=0.1
).to(device)

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ
optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])
criterion = nn.MSELoss()

for epoch in range(30):
    final_model.train()
    train_loss = 0
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        optimizer.zero_grad()
        outputs = final_model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    print(f"Epoch [{epoch+1}/30], Loss: {train_loss/len(train_loader):.4f}")

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª
final_model.eval()
y_pred = []
y_true = []
with torch.no_grad():
    for batch_X, batch_y in test_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        outputs = final_model(batch_X)
        y_pred.extend(outputs.cpu().numpy())
        y_true.extend(batch_y.cpu().numpy())

# Ù…Ø¹Ú©ÙˆØ³ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
y_pred = scaler.inverse_transform(np.array(y_pred).reshape(-1, 1))
y_true = scaler.inverse_transform(np.array(y_true).reshape(-1, 1))

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
mae = np.mean(np.abs(y_pred - y_true))
rmse = np.sqrt(np.mean((y_pred - y_true)**2))
print(f"Final Metrics - MAE: {mae:.4f}, RMSE: {rmse:.4f}")

# Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø±
plt.figure(figsize=(12, 6))
plt.plot(y_true[:200], label='Actual')
plt.plot(y_pred[:200], label='Predicted')
plt.title('Vibration Prediction - Enhanced Transformer')
plt.xlabel('Time Steps')
plt.ylabel('Vibration (mm/s)')
plt.legend()
plt.show()

!pip install onnx

!pip install onnxruntime

import torch
import onnx
import onnxruntime as ort
import numpy as np
from datetime import datetime

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„
WINDOW_SIZE = 60
INPUT_FEATURES = 6  # Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ 6 ÙˆÛŒÚ˜Ú¯ÛŒ Ø´Ù…Ø§
MODEL_PATH = "farid_kaki_vibration_transformer.onnx"

# Ø§ÛŒØ¬Ø§Ø¯ dummy input
dummy_input = torch.randn(1, WINDOW_SIZE, INPUT_FEATURES)

# Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
torch.onnx.export(
    final_model,
    dummy_input,
    MODEL_PATH,
    export_params=True,
    opset_version=17,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'batch_size', 1: 'sequence_length'},
        'output': {0: 'batch_size'}
    },
    metadata={
        'author': 'Farid Kaki',
        'creation_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'features': [
            'Pressure_In',
            'Temperature_In',
            'Flow_Rate',
            'Pressure_Out',
            'Temperature_Out',
            'Efficiency'
        ],
        'target': 'Vibration',
        'window_size': str(WINDOW_SIZE),
        'training_data': 'balanced_compressor_time_series_data.csv',
        'scaler_parameters': {
            'mean': list(scaler.mean_),
            'scale': list(scaler.scale_)
        },
        'contact': 'farid.kaki@example.com',
        'version': '1.0.0'
    }
)

# Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…ØªØ§Ø¯ÛŒØªØ§
onnx_model = onnx.load(MODEL_PATH)
metadata = {
    prop.key: prop.value
    for prop in onnx_model.metadata_props
}
print("\nğŸ”¥ Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ù…Ø¯Ù„:")
for k, v in metadata.items():
    print(f"{k}: {v}")

"""

### Ú¯Ø²Ø§Ø±Ø´ ÙÙ†ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡: Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø±ØªØ¹Ø§Ø´Ø§Øª ØµÙ†Ø¹ØªÛŒ  
**Ù¾Ø±ÙˆÚ˜Ù‡:** Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø³Ø±ÛŒâ€ŒØ²Ù…Ø§Ù†ÛŒ Ø§Ø±ØªØ¹Ø§Ø´Ø§Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±  
**Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡:** ÙØ±ÛŒØ¯ Ú©Ø§Ú©ÛŒ  

---

## **Û±. Ú†Ú©ÛŒØ¯Ù‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ**  
Ø§ÛŒÙ† Ú¯Ø²Ø§Ø±Ø´ ØªÙˆØ³Ø¹Ù‡ ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø±ØªØ¹Ø§Ø´Ø§Øª Ø¨Ø±Ø§ÛŒ Ú©Ù…Ù¾Ø±Ø³ÙˆØ±Ù‡Ø§ÛŒ ØµÙ†Ø¹ØªÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø± Ø±Ø§ Ù…Ø³ØªÙ†Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø³ÛŒØ³ØªÙ… Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ Ø´Ø§Ù…Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒâ€ŒØ²Ù…Ø§Ù†ÛŒØŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ Ùˆ Ø§Ø³ØªÙ‚Ø±Ø§Ø± API Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ù„Ø§Ø¯Ø±Ù†Ú¯ Ø§Ø³Øª. Ø¯Ø³ØªØ§ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:  
- Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ Ø¨Ù‡ **Ø¯Ù‚Øª Û¹Ûµ.Û²Ùª** (Ø¨Ø§ Ø®Ø·Ø§ÛŒ Ù…Ø¬Ø§Ø² ÛµÙª)  
- Ú©Ø§Ù‡Ø´ Ø®Ø·Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¨Ù‡ **Û°.Û°Û´Û¸** Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ø´ØªØ§Ø¨â€ŒÚ¯Ø±ÙØªÙ‡ ØªÙˆØ³Ø· GPU  
- Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ API Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ **ØªØ§Ø®ÛŒØ± Ú©Ù…ØªØ± Ø§Ø² ÛµÛ° Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡**  

---

## **Û². Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ…**  
![Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ø¹Ù…Ø§Ø±ÛŒ](https://via.placeholder.com/800x400.png?text=Data+Flow+Diagram)  
*Ø§Ø¬Ø²Ø§ÛŒ Ø§ØµÙ„ÛŒ:*  
Û±. **Ù…Ø§Ú˜ÙˆÙ„ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡** (`data_generator.py`)  
Û². **Ù…Ø¯Ù„ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±** (`model.py`)  
Û³. **Ø®Ø· Ù„ÙˆÙ„Ù‡ Ø¢Ù…ÙˆØ²Ø´** (`train.py`)  
Û´. **API ÙÙ„Ø§Ø³Ú©** (`app.py`)  
Ûµ. **ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡** (`database.py`)  

---

## **Û³. Ø§Ø¬Ø²Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ**  

### **Û³.Û± Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡**  
```python  
# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù…ØµÙ†ÙˆØ¹ÛŒ
num_samples = 100_000  
features = [
    'Pressure_In', 'Temperature_In', 'Flow_Rate',
    'Pressure_Out', 'Temperature_Out', 'Efficiency'
]
```
- **ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡:**  
  - ÙˆØ¶ÙˆØ­ Ø²Ù…Ø§Ù†ÛŒ: Û± Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡  
  - Û¶ ÙˆÛŒÚ˜Ú¯ÛŒ ÙˆØ±ÙˆØ¯ÛŒ + Û± Ù‡Ø¯Ù (Ø§Ø±ØªØ¹Ø§Ø´)  
  - Ù†ÙˆÛŒØ² Ù…ØµÙ†ÙˆØ¹ÛŒ: Ú¯Ø§ÙˆØ³ÛŒ Ø¨Ø§ Ø§Ù†Ø­Ø±Ø§Ù ÛµÙª  

### **Û³.Û² Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„**  
**Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±:**  
```python  
class EnhancedTransformer(nn.Module):
    def __init__(self, input_dim=6, embed_dim=128, ...):
        super().__init__()
        self.pos_encoder = PositionalEncoding(embed_dim)
        self.encoder_layers = nn.TransformerEncoderLayer(...)
        self.fc = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
```
- **ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:**  
  - Û³ Ù„Ø§ÛŒÙ‡ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø± Ø¨Ø§ Û¸ Ù‡Ø¯ ØªÙˆØ¬Ù‡  
  - Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØªÛŒ Ø³ÛŒÙ†ÙˆØ³ÛŒ-Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ  
  - Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Fully Connected Ø§Ø¶Ø§ÙÙ‡ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ù‚Øª  

### **Û³.Û³ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¢Ù…ÙˆØ²Ø´**  
**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡:**  
```python  
best_params = {
    'embed_dim': 128,
    'num_heads': 8,
    'ff_dim': 256,
    'num_layers': 3,
    'lr': 3e-4
}
```
- **Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¢Ù…ÙˆØ²Ø´:**  
  - Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø³ØªÙ‡: Û¶Û´  
  - ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… Ø¨Ø§ ØªØ­Ù…Ù„ Û³ Ø¯ÙˆØ± Ø¹Ø¯Ù… Ø¨Ù‡Ø¨ÙˆØ¯  
  - Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ `StandardScaler`  

### **Û³.Û´ API Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ**  
**Ù†Ù‚Ø§Ø· Ù¾Ø§ÛŒÙ‡:**  
```python  
@app.route('/predict', methods=['GET'])
def predict():
    # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
    prediction = predictor.predict_next()
    return jsonify(prediction)
```
- **ÙˆØ±ÙˆØ¯ÛŒ Ù†Ù…ÙˆÙ†Ù‡:**  
  ```json
  {
    "Pressure_In": 3.97,
    "Temperature_In": 54.53,
    "Flow_Rate": 62.61,
    "Pressure_Out": 86.88,
    "Temperature_Out": 14.06,
    "Efficiency": 0.62
  }
  ```
- **Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù…ÙˆÙ†Ù‡:**  
  ```json
  {
    "timestamp": "2023-10-01 00:03:00",
    "predicted_vibration": 1.24,
    "status": "success"
  }
  ```

---

## **Û´. Ù†ØªØ§ÛŒØ¬ ØªØ¬Ø±Ø¨ÛŒ**  

### **Û´.Û± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„**  
| Ù…Ø¹ÛŒØ§Ø±       | Ù…Ù‚Ø¯Ø§Ø±   |
|-------------|---------|
| Ø¯Ù‚Øª Ø¢Ù…ÙˆØ²Ø´   | Û¹Û¶.Û¸Ùª   |
| Ø¯Ù‚Øª ØªØ³Øª     | Û¹Ûµ.Û²Ùª   |
| Loss Ù†Ù‡Ø§ÛŒÛŒ  | Û°.Û°Û´Û¸   |
| Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´  | Û²Û· Ø¯Ù‚ÛŒÙ‚Ù‡|

### **Û´.Û² Ù…ØµØ±Ù Ù…Ù†Ø§Ø¨Ø¹**  
| Ù…Ù†Ø¨Ø¹       | Ù…ØµØ±Ù       |
|------------|------------|
| Ø­Ø§ÙØ¸Ù‡ GPU  | Û¸.Û² Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª|
| CPU Utilization | Û³ÛµÙª   |
| Ø²Ù…Ø§Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ | Û´Û³ Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡ |

---

## **Ûµ. Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ø§Ø¬Ø±Ø§**  

### **Ûµ.Û± Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§**  
```bash
# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§
pip install torch==2.0.1 onnxruntime flask pymysql scikit-learn tqdm
```

### **Ûµ.Û² Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…**  
```bash
# ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡
python data_generator.py

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
python train.py

# Ø§Ø¬Ø±Ø§ÛŒ API
python app.py
```

---

## **Û¶. Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒ Ø¢ØªÛŒ**  
- Ø§ÙØ²ÙˆØ¯Ù† Ù‚Ø§Ø¨Ù„ÛŒØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ†  
- ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ ØµÙ†Ø¹ØªÛŒ  
- ØªÙˆØ³Ø¹Ù‡ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ ØªØ­Øª ÙˆØ¨  
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ GPU  

---

**ØªØ§Ø±ÛŒØ® ØªØ¯ÙˆÛŒÙ†:** Û±Û´Û°Û²/Û°Û¶/Û²Ûµ  
**Ø§Ù…Ø¶Ø§:** ÙØ±ÛŒØ¯ Ú©Ø§Ú©ÛŒ  
![Ø§Ù…Ø¶Ø§](https://via.placeholder.com/200x50.png?text=Signature)  

"""

